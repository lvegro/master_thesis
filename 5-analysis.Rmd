---
output: pdf_document
---

# Application to a real-world network

```{r echo=FALSE, message=FALSE}
# save parameters for later
par_default <- par()
library(igraph)
library(RColorBrewer)
library(dplyr)
library(chron)
library(lubridate)
library(xtable)
library(gSeg)
library(cccd)
options(xtable.comment=F)
load(file="../R/data.RData")
load(file="../R/graphs.RData")
```


## Dataset description

In order to present and test the effectiveness of the methods and techniques we 
proposed thus far, we will apply them to the analysis of the data collected by 
the Human Dynamics research group of the Massachusetts Institute of 
Technology (MIT) during the SocialEvolution experiment.  In the period between
October 2008 and May 2009, a group of 84 students and teaching staff residing in
the MIT campus buildings where provided with cellphones that would, in the 
course of the following months, collect data about their communication habits,
their movement and their social interactions with other subjects; in order to do
so the devices kept track of the subjects' call and message log, of their
position relative to a number of Wi-Fi hot-spots and of the meetings via a 
short-range BlueTooth emitter-receiver. 

Together with the sensor data, the subjects' activity was monitored with a 
number of sociometric surveys during the experiment, thus allowing data
collection on a wide variety of issues: from health (depression, dietary 
preferences, flu symptoms), to relationship status with other subjects, to 
political opinions, to musical track sharing from a large database. Those surveys were repeated
periodically during the whole experiment, some of them monthly while other on a
more irregular schedule (two, three or four months apart) [@madan2010social].

The richness of the data set, with its multiple surveys on different topics, also
makes it an ideal candidate to compare the efficacy of various visualization
solutions. Letting for the moment aside the call network we'll later construct, 
we deem appropriate to give an example of the kind of information contained in
the data.

The health survey was repeated on `r length(unique(health$survey.month))` distinct
occasions. Its aim was to measure several indicators of healthy lifestyle, such as
regular exercise, diet rich with fruits and vegetables, smoking habits, together
with  weight and height of the subjects to monitor their body mass index. Since
the general aim of the study was to explore the spread of behavior in a closed
social group, it's understandable the interest in the adoption of healthier
habits.

We present a few simple descriptive
tables to give an idea of the level of variability observed in this
area during the study.

```{r echo =FALSE, message=F, warning=F, cache=TRUE}
get.table <- function(var){ # since dplyr uses nonstandard evaluation we need the standard
                    # version of the functions, with a final underscore
df <- health %>% group_by_("survey.month", var) %>%
  tally() %>%
  mutate(rel.freq = (round(100* n/sum(n), 1))) %>% select(-n)
df <- reshape(as.data.frame(df), idvar = "survey.month",
        timevar=var, direction="wide")
rownames(df) <- df$survey.month; df <- df[-1]; 
names(df) <- gsub(pattern = "^rel.freq.", "", names(df) ) 

return(df)
}

```


```{r fruit, echo=F, results="asis", message=F, warning=F, cache=TRUE}
var <- names(health)[5] # veggies
tab <- get.table(var)
tab[is.na(tab)] <-  0
tab$fiveplus <- (tab$`5` +  tab$`6`+tab$`7`+ tab$"NA")
tab <- tab[c(2,1,3,4,5,6),c(1,2,3,4, 5, 10)]
names(tab) <- c("1", "2", "3", "4", "5+")
 
print(xtable(tab, caption = "Fruit or vegetables eaten per day, in percent of total subjects"))
```

```{r diet, echo =F, results="asis", message=F, warning=F}
 var <- names(health)[6] # healthy diet
 tab <- get.table(var)
 tab[is.na(tab)] <-  "-"
 tab <- tab[c(2,1,3,4,5,6),c(6, 4, 2, 1, 3, 5)]
 print(xtable(tab, caption = "Healthiness of diet, in percent of total subjects (self reported, Likert scale)"))
```

```{r sport, echo =F, results="asis", message=F, warning=F, cache=TRUE}
 var <- names(health)[8] #sport per week
 tab <- get.table(var)
 tab[is.na(tab)] <- "-"
 tab <- tab[c(2,1,3,4,5,6), c(1,2,3,4, 8,5,6)]
 print(xtable(tab, caption="Sport per week in percent of total subjects"))
```

```{r smoking, echo =F, results="asis", message=F, warning=F, cache=TRUE}
 var <- names(health)[9]
 tab <- get.table(var)
 tab[is.na(tab)] <- "-"
 tab <- tab[c(2,1,3,4,5,6), c(4,5,6,7)]
 names(tab) <- c(names(tab)[1:3], "Other")
 print(xtable(tab, caption="Currently smoking status in percent of total subjects"))
```

There is a clear trend of adoption of healthier habits during the
whole observation period. In fact, @madan2010social found that eating behavior and
attitude to exercise are adopted more readily when there is frequent contact
with individuals who already exhibit such behavior. Using data from the
Social Evolution study, The authors also found that contact registered through
remote sensors is a better predictor or this adoption than self-reported
acquaintance with peers and habits.

Among the efforts that were made, during the data collection phase,
to monitor the health state of the community, flu symptoms were tracked via a
specific survey held during the winter months, on a daily basis.
Obviously since the movement of the subjects and their social activity during
this period is known, the attempts at modeling the infection within the
community can take advantage of this information too.

```{r flu, echo = F,  fig.cap="Total number of flu-like symptoms reported by the subjects", fig.scap="Trend of reported flu symptoms", warning=F, message=F, cache=TRUE, fig.align='center'}
flu_symptoms$day <- as.Date.character(substr(flu_symptoms$time_stamp,start = 1, stop = 10))
symptoms <- rowsum.data.frame(group = flu_symptoms$day,
                              flu_symptoms[,c("sore.throat.cough",
                                              "runnynose.congestion.sneezing",
                                              "fever",
                                              "nausea.vomiting.diarrhea")])
nsymptoms <- ts(rowSums(symptoms))
plot(nsymptoms, col="cornflowerblue",
     ylab = "# of symptoms reported",
     xlab="Time in days since 20 Dec. 2008")
```

This section is intended only as an illustration of the richness of the
data collected during the experiment. In fact, many studies have been conducted
exploring all the possible connection and predictive analysis possible given
the variety of information available to researcher. It is worth noting
how this was achieved in 2008-2009, and therefore how much more could be
possible nowadays given the evolution of technological means for remote data
collection.

## Exploratory Analysis

In the following section we shall begin exploring the characteristic of our subjects
and their call patterns, following with an exploratory analysis of the networks
themselves. As we said before, the subjects were mainly students residing in the
MIT campus, accounting for more than 80\% of the population of the campus at that
time.

The provided cellphones during that time tracked a total of `r dim(calls)[1]`
phone calls; of those calls, `r dim(subset(calls, !is.na(dest_user_id_if_known)))[1]`,
or `r format(dim(subset(calls,!is.na(dest_user_id_if_known)))[1]/dim(calls)[1]*100, digits=4)`\%
were directed to other subjects in the study. We shall concentrate on those
calls exclusively.

As a first step is is of a certain interest to analyze how
these calls are distributed in the temporal dimension: how many calls are made during
work hours or in the evening; during workdays and on weekends, etc.

```{r echo =F}
# since we don't need to work w/ external calls we can overwrite the original data.frame
calls <- subset(calls, subset = !is.na(calls$user_id) & !is.na(calls$dest_user_id_if_known))
den <- dim(calls)[1]
```

```{r fig=TRUE, echo=F, message=F, warning=F}
# fix axis and colors
par(mfrow=c(1,2))

plot(table(wday(chron(calls$time_stamp)))/den, main="Percent of calls on each day",
     xlab = "Day of the week", ylab = "Percent of total calls")
# -5 %% 24 to fix timezone!
plot(table((hour(chron(calls$time_stamp))-5) %% 24)/den,
     main = "Percent of total calls by hour",
     xlab = "Hour", ylab = "Percent of total calls",
     call="lightgrey")
par(mfrow=c(1,1))
```

We can see that the day of the week does not appear to be particularly relevant,
as the calls seem uniformly distribute from Sunday to Monday (note that 1 = Sunday,
in this case). The hourly distribution
of the calls is also unsurprising, with calls becoming more frequent after 8 a.m,
reaching their peak in the evening and than starts decreasing during the night.

```{r echo = F, cache=TRUE}
tmp <- rbind(as.data.frame(table(calls$user_id)),
             as.data.frame(table(calls$dest_user_id_if_known)))

tmp <- with(tmp,(aggregate(Freq~Var1, FUN=sum)))
most_active <- tmp[order(tmp$Freq, decreasing = T),]
row.names(most_active) <- as.character(1:dim(most_active)[1])
```

We then want to identify which users are more active and therefore more important to the
definition of the network structure.
We chose to define user activity by counting the number of calls in which a
subject was either calling or being called. It's easy to see, looking at 
\autoref{topcallers}, that the five most active subjects account for almost 40\% of
all activity in the network.
Intuitively we can presume that the structure of our networks will then be very
dependent on the activity of those few subjects.

```{r fig = T, echo = F, out.width=".60\\linewidth", fig.align='center', cache=TRUE, fig.cap="\\label{topcallers}Top 6 callers in the network"}
barplot(rev(most_active$Freq[1:6]/sum(most_active$Freq)),
        main = "Percent of total activity for each user",
        horiz=T,names.arg = most_active$Var1[1:6])
```

### Building and visualizing the networks

We consider phone calls between subjects as indicative of a social link.
A network graph is constructed, for each day of observation, linking two
subjects (the nodes of the graph) with a edge if there was at least one call
between those two subjects on that day. 

The edges are undirected, so calls from a subject A to a subject B are
equivalent to those coming from B to A. It's important to note that, while the
original data did include information about the duration of the calls and it
would have seemed appropriate to use that information to weight the strength of
the ties, the duration data was of very low quality and with errors of various
kinds that made it, on all accounts, unusable. Several calls were listed as
having negative or 0 duration, and By excluding
the information on call duration we are forced to consider the contacts between
subjects as instantaneous, albeit with a daily resolution.
This is a gross simplification, that is nonetheless widely used in the available
literature [@holme2012temporal].
The distribution of call duration (not reported herein) showed some regularities
that probably corresponded to the duration of calls that were dropped when the
answering machine mobile service kicked in. Unfortunately documentation about
the data was not made available and the lack of information made it impossible
to properly use the call duration variable.

A justification for this admittedly naive
solution can be at any rate a phone callexpresses
a need or will to communicate and it's therefore indicative of a social
connection, regardless of the fact that the call was actually received.

Moreover, such a simple structure has the advantage that it makes it easier to
to focus on the true point of interest of a longitudinal
analysis: the dynamic behavior of the network.

Before listing descriptive statistics and  characteristics of the
networks, it's no doubt useful to visualize it. We chose the simplest possible
representation, plotting all the nodes as points along a circle and connecting
them with straight lines. An example can be seen in \autoref{aggr}.
This kind of layout is also advantageous in animated plots, as the position
of the vertices in the plot can be held constant through all the observation
period and allows to spot clearly connection between pairs of subjects.
Note that, to achieve this goal, the order in which the vertices are plotted
must be chosen so to minimize contact between adjacent vertices, which would
then be very hard to notice, being hidden by the shape used to plot the vertices
themselves.

While simple, this type of representation
also has the advantage of being easily
extended to include other information about the subjects or the connections,
intuitively symbolized by coloring the nodes and edges in different ways and
plotting them at different sizes. The space at the center of the plot can 
become cluttered if there are too many edges, but this is easily solved by
letting the oldest connection fade as time progresses, as can be seen in the
animated example.

```{r echo=FALSE, fig.scap="Daily and aggregated representations of the network", fig.cap="\\label{aggr}Here we show, from left to right and top to bottom, the first five daily snapshots of the network. In the bottom right position we can observe the aggregated graph obtained by collating the previous snapshots. The color and width of the edges provide a visual help in assessing the frequency of the connections between subjects", warning=F, cache=F, fig.align='center'}

par(mfrow=c(2,3),
          oma = c(0.2,0.2,0,0) + 0.1,
          mar = c(0,0,0.2,0.2) + 0.1)
for(i in 2:6){plot(graphs[[i]],
                   layout=layout_in_circle(graphs[[i]],
                                           order = sort(V(graphs[[i]])$name)),
                   edge.color="cornflowerblue", edge.width=1, vertex.label=NA,
                   vertex.size=7, vertex.color="light grey")}
plot(cumgraphs[[6]],
     layout=layout_in_circle(cumgraphs[[6]],
                            order = sort(V(cumgraphs[[6]])$name)),
                   edge.color=brewer.pal(5, "Blues")[E(cumgraphs[[6]])$weight],
                  edge.width=E(cumgraphs[[6]])$weight, vertex.label=NA,
                   vertex.size=7, vertex.color="light grey")

par(par_default)
```


```{r echo =F, fig=T, fig.scap="Network at day 60",fig.cap="By observing the aggregated network at day 60 it becomes obvious how the connectivity of the aggregated graph is an overestimation of the actual connections between subjects on any given day. Regularities in the daily call pattern become impossible to identify, because of the *ball of yarn* phenomenon.", cache=F, fig.align='center'}
n <- 60
plot(cumgraphs[[n]],
     layout=layout_in_circle(cumgraphs[[n]],
                             order=sort(V(cumgraphs[[n]])$name)),
     edge.color="light blue",
     edge.width=E(cumgraphs[[n]])$weight,
     vertex.color="light grey",
     vertex.label=NA,
     vertex.size=8)
```

To better appreciate the difference that emerges when comparing results obtained
analyzing the daily snapshot of the network to that of the aggregated graph on the
final day, we will start with the usual static analysis of the
"final day"" network.

```{r echo =F, fig =T, fig.scap='Aggregate state of the network at end of the observation period', fig.cap="\\label{finalstate}The network in its final aggregated state. Each circle represents a node, colored according to their academic status at the start of the experiment", fig.align='center', cache=TRUE}
set.seed(123)
GG <- cumgraphs[[279]]
pal <- categorical_pal(length(unique(V(GG)$year)))
names(pal) <- unique(V(GG)$year)
plot(GG, vertex.size=7, vertex.color=pal[V(GG)$year], vertex.label=NA)
legend("topleft", legend=names(pal), col= pal, pch=20, pt.cex=2, bty="n")
```

In figure \autoref{finalstate} we can see that the most connected
vertices, near the center, correspond to sophomore students. Seniors and freshmen
in the periphery indicate they interact in more limited communities, clearly
seen from the cluster of nodes of the same colors.

Another important factor in the dynamics of our network is number of subjects
that actually contribute to the call pattern day by day. In \autoref{activity}
we plotted the fraction of users inactive over time.

```{r fracmissing, echo=F}
##to compute the fraction of actors missing, daily
missing <-
  1 - unlist(lapply(graphs, function(g) {
    sum(degree(g) != 0) / length(V(g))
  }))
# The number of partecipating actors vaires over the time frame, with a tendency
# towards a decrease in partecipation throughout the first half of the study
# culminating around days 165-185, when the network almost went dark except for
# a few calls between some pairings of subjects.
```

It must be noted that the number of subjects contributing to the network varies
greatly from one day to the next, as is apparent by the erratic behavior of the
grey line. The smoother however appears able to capture the underlying tendency: participation
starts low, remains relatively stable around an average of 30\% until day 175,
when it starts decreasing again. In the last few observations the networks are
very small and there are a few days in which only 2/3 people interact.

Another interesting descriptive statistics is the average number of connections 
established by active users daily, vs. time. By plotting the two time series
together, fraction of user inactive and average number of connection of active
users, we cannot however discern any particular pattern. The two values appear
be negatively correlated, $\rho = -0.33$.

```{r echo =FALSE, fig.cap="\\label{activity}Trends in participation and activity in the network", fig.align='center'}
nconn <- sapply(graphs, function(g) return(length(E(g))))
nuser <- sapply(graphs, function(g) return(sum(degree(g)!=0)))
#hist(nconn)
#hist(nuser)
scatter.smooth(
  x = missing, type = "l",
   xlab = "Day", ylab =
    "Fraction Missing",
  col = "dark grey", lwd = 0.85, span = 1 / 4,
  lpars = c(col = "red", lwd = 2), ylim = c(.5,1)
)
lines(ts(nconn/nuser))
```
We may try to *thin* our graphs, by excluding those days in which the
network was almost inactive: a possible choice is that of excluding all graphs
whose number of connection falls below a given percentile of the distribution
of the daily number of edges. Should we opt to exclude those which fall under the
first decile, for example, we would need to exclude all graphs with four
connections or less.

```{r histandF, echo=FALSE, message=F, results="hide", out.width=".85\\linewidth", fig.scap="Number of daily connections", fig.cap="Cumulative distribution function and histogram of the number of daily connections recorded", fig.align='center'}
par(mfrow=c(1,2))
Fu <- ecdf(nconn)
plot(Fu, main = "", xlab="Number of connections")
hist(nconn, main = "", col = "lightgrey", breaks=24, xlab="Number of connections")
par(mfrow=c(1,1))
```


### Centrality Measures
As summarized in @kolaczyk2014statistical, the effort of capturing certain
structural characteristics of networks is generally pursued by analyzing the properties of their vertices
and edges, the constituting components of any network.

According to the observations made in chapter 2, degree centrality can be interpreted
as the popularity or connectedness in the widest sense of a vertex, betweenness centrality in
the tendency for a vertex to act as a relay for information, and closeness centrality as
an independence from other vertices in communicating information to neighbors.

With that in mind, we propose a visual representation of these three centrality
measures on the final aggregated graph, and a selection of descriptive statistics
focusing on degree centrality. An interesting aspect to note is that the most central
actor according to these measures is the one with ID 44, which however ranks only
18th when considering the total number of calls made or received. Thus sheer
levels of activity do not automatically translate into structural relevance, and therefore it becomes necessary to investigate the variation in time of the
call pattern.

```{r, echo =F, fig.align='center', fig.scap="Average degree centrality", fig.cap="\\label{avgdegree}Average degree centrality (normalized) by day with superimposed LOESS smoother"}
###################################################
### code chunk number 15: analysis.Rnw:364-376
###################################################
par(mfrow = c(1,1))
deg <- unlist(lapply(dg, function(g) {
  return(mean(degree(g, normalized = T)))
}))

scatter.smooth(
  x = deg[-which(deg==1)], type = "l",
  xlab = "Day", ylab = "Avg. Degree",
  col = "dark grey", lwd = 0.85, span = 1 / 4,
  lpars = c(col = "red", lwd = 2)
)
```

We present below the values of the correlation between the network graph-wide centrality
measures. They appear to be highly correlated, which is not unusual and is in fact
a common result both at vertex and at graph level.

\begin{center}
```{r, echo=F, message=F, cache=F, results='asis'}
###################################################
### code chunk number 16: analysis.Rnw:391-401
###################################################
par(mfrow = c(1,1))
btw <- unlist(lapply(dg, function(g) {
  return(mean(betweenness(g, normalized = T)))
}))

cls <- unlist(lapply(dg, function(g) {
  return(mean(closeness(g, normalized = T)))
}))

M <- (cbind(deg[!is.na(btw)], btw[!is.na(btw)], cls[!is.na(btw)]))


###################################################
### code chunk number 17: corr
###################################################
M <- cor(M)
colnames(M) <- rownames(M) <- c("Dgr", "Btwnss", "Clsnss")

print(xtable(M, caption = "Correlation between centrality measures"), 
             label = "corrcent")
```
\end{center}

```{r echo=F, message=F, cache=F, results='asis'}
###################################################
### code chunk number 18: analysis.Rnw:414-446
###################################################
x<- lapply(dg, FUN = function(g){
  v <- degree(g, normalized = T)
  m <- max(v)
  n <- length(V(g))
  denom <- n^2 + (3*n) - 2
  return(sum((m - v))/denom)
})

net_deg_c <- 1- unlist(x)


x<- lapply(dg, FUN = function(g){
  v <- betweenness(g, normalized = T)
  m <- max(v)
  n <- length(V(g))
  denom <- n^3 - (4*(n^2)) + 5*n - 2
  return(sum((m - v))/denom)
})

net_btw_c <- unlist(x)
net_btw_c <- subset(net_btw_c, !is.nan(net_btw_c))
net_btw_c <- subset(net_btw_c, net_btw_c!=max(net_btw_c))

x<- lapply(dg, FUN = function(g){
  v <- closeness(g, normalized = T)
  m <- max(v)
  n <- length(V(g))
  denom <- (n^2 - (3*(n)) + 2)/(2*n - 3)
  return(sum((m - v))/denom)
})

net_cls_c <- unlist(x)


###################################################
### code chunk number 19: analysis.Rnw:452-493
###################################################
# example for the degree centrality measure, extensions and modifications
# are immediate:

# list with each element a vector containing all computed measures
# measure takes values "degree", "closeness" or "betweenness"
centgraphs <- function(measure){
  
  
  measuref <- getFunction(measure)
  v <- lapply(dg[!is.na(btw)], function(g) {
  return(measuref(g, normalized = T))
  })
  max <- lapply(v, function(v) {
  return(max(v))
  })
  # from measure vector and max, compute vector of differences
  diff <- vector(mode = "list")
  
  for (i in 1:length(v)) {
  diff[[i]] <- max[[i]] - v[[i]]
  }
  
  # last step requires computing the sum of all the vectors contained in diff
  # and scale them with the appropriate normalizing factor:
  
  helper <- function(v) {
  n <- length(v)
  if (measure == "degree") {
  den <- n^2 - 3*n + 2 # fill w/correct
  } else if (measure == "betweenness") {
  den <- (n^3) -(4*(n^2)) +(5*n) -2
  } else if (measure == "closeness") {
  den <- (n^2 -3*n + 2)/(2*n - 3)
  }
  return(sum(v) / den)
  }
  
  res <- (lapply(diff, FUN = function(v){helper(v)}))
  return(unlist(res))
}



###################################################
### code chunk number 20: analysis.Rnw:496-499
###################################################
Cb <- lapply(graphs, betweenness)
# list w/ named vectors
Cb <- lapply(Cb, function(x){return(x[order(names(x))])})
```

```{r centrality_display, fig=T, echo=F, fig.scap="Comparison between centrality measures for each vertex on the aggregate network", fig.cap="Comparison between centrality measures for each vertex, considering the whole aggregate network From left to right, Degree, Betwenness and Closeness", out.width='.80\\linewidth', fig.align='center', warning=F, fig.align='center'}
 par(mfrow=c(1,3),
           oma = c(0.2,0.2,0,0) + 0.1,
           mar = c(0,0,0.2,0.2) + 0.1)
#need to set the seed since the layout is partially random
set.seed(123)
pal <- brewer.pal(7, "Blues")
cuts <- cut(degree(GG, normalized = T), breaks=7)
names(pal)=levels(cuts)
plot(GG, vertex.color=pal[cuts], vertex.size=7, vertex.label=NA)
set.seed(123)
pal <- brewer.pal(7, "Greens")
cuts <- cut(betweenness(GG, normalized = T), breaks=7)
names(pal)=levels(cuts)
plot(GG, vertex.color=pal[cuts], vertex.size=7, vertex.label=NA,
     layout=layout_nicely(GG))
set.seed(123)
pal <- brewer.pal(7, "Reds")
cuts <- cut(closeness(GG, normalized = T), breaks=7)
names(pal)=levels(cuts)
plot(GG, vertex.color=pal[cuts], vertex.size=7, vertex.label=NA,
     layout=layout_nicely(GG))
# restore default graphic parameters
par(par_default)
```


## Changepoint detection

@chen2015graph proposed a method that requires a suitable distance metric
to be defined on the data. In Chen's own example this was done on a cellphone
network by counting, day after day, the number of different edges, that is
the Frobenius norm of the difference of two consecutive adjacency matrices, or
alternatively with a normalized version of the same measure.

We'll then have a number of nodes equal to that of the observations and we'll
know the distance between those nodes. We'll therefore be able to construct
a graph connecting those node according to some rules -- we might for
example decide to minimize the total distance spanned by the edges and
build therefore a Minimum Spanning Tree (MST). Alternative we might chose to
consider each vertex connected to the $K$ closest one, building a Nearest Neighbor
Graph (NNG) of order $K$.
We shall attempt first to replicate this procedure and then to analyze the sensitivity of the
method to changes in the distance measure and in the parameters of the analysis,
for example the choice of the scan window, i.e. the time frame in which to search
for the change-point $\tau$.

It's important to note that our adjacency matrices are square matrices of order 
`r length(V(graphs[[1]]))` and generally sparse (most of the elements are equal to 0).
Computation time therefore will be rather high, given these characteristics.

```{r gsegsilent, echo=F}

# Silent version of `gseg1`

gseg1 <- function (n, E, n0 = 0.05 * n, n1 = 0.95 * n, pval.appr = TRUE, 
    skew.corr = TRUE, pval.perm = FALSE, B = 100) 
{
    n0 = ceiling(n0)
    n1 = floor(n1)
    Ebynode = vector("list", n)
    for (i in 1:n) Ebynode[[i]] = rep(0, 0)
    for (i in 1:nrow(E)) {
        Ebynode[[E[i, 1]]] = c(Ebynode[[E[i, 1]]], E[i, 2])
        Ebynode[[E[i, 2]]] = c(Ebynode[[E[i, 2]]], E[i, 1])
    }
    r1 = gcp1bynode(n, Ebynode, n0, n1)
    results <- c()
    results <- c("Estimated change-point location:", r1$tauhat, "\n")
    results <- c(results,c("Zmax:", r1$Zmax, "\n"))
    if (pval.appr == TRUE) {
        mypval1 = pval1(n, E, Ebynode, r1$Zmax, skew.corr, n0, 
            n1)
        r1$pval.appr = min(mypval1, 1)
    #    cat("Approximated p-value:", r1$pval.appr, "\n")
    }
    if (pval.perm == TRUE) {
        mypval2 = permpval1(n, Ebynode, r1$Zmax, B, n0, n1)
        r1$pval.perm = min(mypval2$pval, 1)
        r1$perm.curve = mypval2$curve
        r1$perm.maxZs = mypval2$maxZs
        r1$perm.Z = mypval2$Z
    #   cat("p-value from", B, "permutations:", r1$pval.perm, 
    #       "\n")
    }
    return(r1)
}

# the Nu function
Nu = function(x){ 
  y = x/2
  (1/y)*(pnorm(y)-0.5)/(y*pnorm(y) + dnorm(y))
}

# single change-point
gcp1bynode = function(n, Ebynode, n0=ceiling(0.05*n), n1=floor(0.95*n)){
# "n" is the total number of nodes.
# Ebynode[[i]] is the list of nodes that are connect to i by an edge.
# The nodes are numbered by their order in the sequence.  
# To estimate the change-point, we find the maximum of Z(t), the standardized
# version of R(t), between n1 and n2. 
  g = rep(1,n)
  R = rep(0,n)
  for(i in 1:n){
    g[i] = 0  # update g
    links = Ebynode[[i]]
    if(i==1){
      if(length(links)>0){
        R[i] = sum(rep(g[i],length(links)) != g[links])
      } else {
        R[i] = 0
      }
    } else {
      if(length(links)>0){
        add = sum(rep(g[i],length(links)) != g[links])
        subtract = length(links)-add
        R[i] = R[i-1]+add-subtract
      } else {
        R[i] = R[i-1]
      }
    }
  }
  tt = 1:n
  nodedeg = rep(0,n)
  for(i in 1:n) nodedeg[i] = length(Ebynode[[i]])
  sumEisq = sum(nodedeg^2)
  nE = sum(nodedeg)/2
  mu.t = nE* 2*tt*(n-tt)/(n*(n-1))
  p1.tt = 2*tt*(n-tt)/(n*(n-1))
  p2.tt = tt*(n-tt)*(n-2)/(n*(n-1)*(n-2))
  p3.tt = 4*tt*(n-tt)*(tt-1)*(n-tt-1)/(n*(n-1)*(n-2)*(n-3))
  A.tt = (p1.tt-2*p2.tt+p3.tt)*nE+(p2.tt-p3.tt)*sumEisq+p3.tt*nE^2
  
  Z = (mu.t-R)/sqrt(A.tt-mu.t^2)
  Z[n] = 0
  
  temp=n0:n1
  tauhat = temp[which.max(Z[n0:n1])]
  
  return(list(tauhat=tauhat,Zmax=Z[tauhat],Z=Z,R=R))
}

# rho_one = n h_G
rho_one = function(n, s, sumE, sumEisq){
  f1 = 4*(n-1)*(2*s*(n-s)-n)
  f2 = ((n+1)*(n-2*s)^2-2*n*(n-1))
  f3 = 4*((n-2*s)^2-n)
  f4 = 4*n*(s-1)*(n-1)*(n-s-1)
  f5 = n*(n-1)*((n-2*s)^2-(n-2))
  f6 = 4*((n-2)*(n-2*s)^2-2*s*(n-s)+n)
  n*(n-1)*(f1*sumE + f2*sumEisq - f3*sumE^2)/(2*s*(n-s)*(f4*sumE + f5*sumEisq - f6*sumE^2))
}


# p value approximation for single change-point
pval1 = function(n, E, Ebynode, Zmax, skew.corr=TRUE, n0=ceiling(0.05*n), n1=floor(0.95*n)){
  b = Zmax
  deg = rep(0,n)
  for(i in 1:n) deg[i] = length(Ebynode[[i]])
  sumE = sum(deg)/2
  sumEisq = sum(deg^2) 
  integrand = function(s){
    x = rho_one(n,s,sumE,sumEisq)
    x*Nu(sqrt(2*b^2*x))
  }
  mypval = dnorm(b)*b*integrate(integrand, n0, n1, subdivisions=3000, stop.on.error=FALSE)$value
  if (skew.corr==FALSE){
    return(mypval)
  }
  x1 = sum(deg*(deg-1))
  x2 = sum(deg*(deg-1)*(deg-2))
  x3 = 0
  for (i in 1:nrow(E)){
    x3 = x3 + (deg[E[i,1]]-1)*(deg[E[i,2]]-1)
  }  
  x4 = sum(deg*(deg-1)*(sumE-deg))
  x5 = 0
  for (i in 1:nrow(E)){
    j = E[i,1]
    k = E[i,2]
    x5 = x5 + length(which(!is.na(match(Ebynode[[j]], Ebynode[[k]]))))
  }
  s = 1:n
  x = rho_one(n,s,sumE,sumEisq)
  p1 = 2*s*(n-s)/(n*(n-1))
  p2 = 4*s*(s-1)*(n-s)*(n-s-1)/(n*(n-1)*(n-2)*(n-3))
  p3 = s*(n-s)*((n-s-1)*(n-s-2) + (s-1)*(s-2))/(n*(n-1)*(n-2)*(n-3))
  p4 = 8*s*(s-1)*(s-2)*(n-s)*(n-s-1)*(n-s-2)/(n*(n-1)*(n-2)*(n-3)*(n-4)*(n-5))
  mu = p1*sumE
  sig = sqrt(apply(cbind(p2*sumE + (p1/2-p2)*sumEisq + (p2-p1^2)*sumE^2, rep(0,n)), 1, max))  # sigma
  ER3 = p1*sumE + p1/2*3*x1 + p2*(3*sumE*(sumE-1)-3*x1) + p3*x2 + p2/2*(3*x4-6*x3) + p4*(sumE*(sumE-1)*(sumE-2)-x2-3*x4+6*x3)- 2*p4*x5
  r = (mu^3 + 3*mu*sig^2 - ER3)/sig^3
  theta_b = rep(0,n)
  pos = which(1+2*r*b>0)
  theta_b[pos] = (sqrt((1+2*r*b)[pos])-1)/r[pos]
  ratio = exp((b-theta_b)^2/2 + r*theta_b^3/6)/sqrt(1+r*theta_b)
  a = x*Nu(sqrt(2*b^2*x)) * ratio
  nn = n-length(pos)
  if (nn>0.75*n){
    return(0)
  }
  if (nn>=2*(n0-1)){
    neg = which(1+2*r*b<=0)
    dif = neg[2:nn]-neg[1:(nn-1)]
    id1 = which.max(dif)
    id2 = id1 + ceiling(0.03*n)
    id3 = id2 + ceiling(0.09*n)
    inc = (a[id3]-a[id2])/(id3-id2)
    a[id2:1] = a[id2+1]-inc*(1:id2)
    a[(n/2+1):n] = a[(n/2):1]
    neg2 = which(a<0)
    a[neg2] = 0
  }
  integrand = function(s){
    a[s]
  }
  result = try(dnorm(b)*b*integrate(integrand, n0, n1, subdivisions=3000, stop.on.error=FALSE)$value, silent=T)
  if (is.numeric(result)){
    return(result)
  }else{
    cat("p value approximation without skewness correction is calculated.\n")
    return(mypval)
  }
}

# p value approximation for changed interval
pval2 = function(n, E, Ebynode, Zmax, skew.corr=TRUE, l0=ceiling(0.05*n), l1=floor(0.95*n)){
  b = Zmax
  deg = rep(0,n)
  for(i in 1:n) deg[i] = length(Ebynode[[i]])
  sumE = sum(deg)/2
  sumEisq = sum(deg^2) 
  integrand = function(s){
    x = rho_one(n,s,sumE,sumEisq)
    (b^2*x*Nu(sqrt(2*b^2*x)))^2*(n-s)
  }
  mypval = dnorm(b)/b*integrate(integrand, l0, l1, subdivisions=3000, stop.on.error=FALSE)$value
  if (skew.corr==FALSE){
    return(mypval)
  }
  x1 = sum(deg*(deg-1))
  x2 = sum(deg*(deg-1)*(deg-2))
  x3 = 0
  for (i in 1:nrow(E)){
    x3 = x3 + (deg[E[i,1]]-1)*(deg[E[i,2]]-1)
  }  
  x4 = sum(deg*(deg-1)*(sumE-deg))
  x5 = 0
  for (i in 1:nrow(E)){
    j = E[i,1]
    k = E[i,2]
    x5 = x5 + length(which(!is.na(match(Ebynode[[j]], Ebynode[[k]]))))
  }
  s = 1:n
  x = rho_one(n,s,sumE,sumEisq)
  p1 = 2*s*(n-s)/(n*(n-1))
  p2 = 4*s*(s-1)*(n-s)*(n-s-1)/(n*(n-1)*(n-2)*(n-3))
  p3 = s*(n-s)*((n-s-1)*(n-s-2) + (s-1)*(s-2))/(n*(n-1)*(n-2)*(n-3))
  p4 = 8*s*(s-1)*(s-2)*(n-s)*(n-s-1)*(n-s-2)/(n*(n-1)*(n-2)*(n-3)*(n-4)*(n-5))
  mu = p1*sumE
  sig = sqrt(p2*sumE + (p1/2-p2)*sumEisq + (p2-p1^2)*sumE^2)  # sigma
  ER3 = p1*sumE + p1/2*3*x1 + p2*(3*sumE*(sumE-1)-3*x1) + p3*x2 + p2/2*(3*x4-6*x3) + p4*(sumE*(sumE-1)*(sumE-2)-x2-3*x4+6*x3) - 2*p4*x5
  r = (mu^3 + 3*mu*sig^2 - ER3)/sig^3
  theta_b = rep(0,n)
  pos = which(1+2*r*b>0)
  theta_b[pos] = (sqrt((1+2*r*b)[pos])-1)/r[pos]
  ratio = exp((b-theta_b)^2/2 + r*theta_b^3/6)/sqrt(1+r*theta_b)
  a = (b^2*x*Nu(sqrt(2*b^2*x)))^2 * ratio
  nn = n-length(pos)
  if (nn>0.75*n){
    return(0)
  }
  if (nn>=2*(l0-1)){
    neg = which(1+2*r*b<=0)
    dif = neg[2:nn]-neg[1:(nn-1)]
    id1 = which.max(dif)
    id2 = id1 + ceiling(0.03*n)
    id3 = id2 + ceiling(0.09*n)
    inc = (a[id3]-a[id2])/ceiling(0.03*n)
    a[id2:1] = a[id2+1]-inc*(1:id2)
    a[(n/2+1):n] = a[(n/2):1]
    neg2 = which(a<0)
    a[neg2] = 0
  }
  integrand = function(s){
    a[s]*(n-s)
  }
  result = try(dnorm(b)/b*integrate(integrand, l0, l1, subdivisions=3000, stop.on.error=FALSE)$value, silent=T)
  if (is.numeric(result)){
    return(result)
  }else{
    cat("p value approximation without skewness correction is reported.\n")
    return(mypval)
  }
}

# p value from permutation for single change point
permpval1 = function(n, Ebynode, Zmax, B=100, n0=ceiling(0.05*n), n1=floor(0.95*n)){
# Computes the pvalue P(max_{n1<=t<=n2} Z(t)>b) by permuting the nodes in the graph.
   Z = matrix(0,B,n)
  for(b in 1:B){
    if(b%%1000 ==0) {
      cat(b, "permutations completed.\n")
    }
    perm = sample(n)
    permmatch = rep(0,n)
    for(i in 1:n) permmatch[perm[i]] = i
    Ebnstar =  vector("list", n)
    for(i in 1:n){
      oldlinks = Ebynode[[permmatch[i]]]
      Ebnstar[[i]] = perm[oldlinks]
    }
    gcpstar=gcp1bynode(n,Ebnstar,n0,n1)
    Z[b,] = gcpstar$Z
  }
  
  maxZ = apply(Z[,n0:n1],1,max)
  maxZs=sort(maxZ)
  p=1-(0:(B-1))/B
  ## if (isplot){
  ##   plot(maxZs,p,type="l")
  ## }
  return(list(pval=length(which(maxZs>=Zmax))/B, curve=cbind(maxZs,p), maxZs=maxZs, Z=Z))
}

# p value from permutation for changed interval
permpval2 = function(n,Ebynode,Zmax,B=100,l0=ceiling(0.05*n),l1=floor(0.95*n)){
# Computes the pvalue for changed interval by permuting the nodes in the graph.  
  Z = matrix(nrow=B,ncol=n*n)
  for(b in 1:B){
    if(b%%100 ==0) {
      cat(b, "permutations completed.\n")
    }
    perm = sample(n)
    permmatch = rep(0,n)
    for(i in 1:n) permmatch[perm[i]] = i
    Ebnstar =  vector("list", n)
    for(i in 1:n){
      oldlinks = Ebynode[[permmatch[i]]]
      Ebnstar[[i]] = perm[oldlinks]
    }
    gcpstar=gcp2bynode(n,Ebnstar,l0,l1)
    Z[b,] = gcpstar$Zv
  }
  dif = matrix(0,n,n)
  for (i in 1:n){
    for (j in 1:n){
      dif[i,j] = j-i
    }
  }
  difv = as.vector(t(dif))
  ids = which((difv>=l0)+(difv<=l1)==2)
  maxZ = apply(Z[,ids],1,max)
  maxZs = sort(maxZ)
  p=1-(0:(B-1))/B
  ## if (isplot){
  ##   plot(maxZs,p,type="l")
  ## }
  return(list(pval=length(which(maxZs>=Zmax))/B, curve=cbind(maxZs,p), maxZs=maxZs, Z=Z))
}
```

```{r gseg_functions, echo=F}
#distance the sum of the difference squared (if two edges are the same,
#0 squared = 0, 1 or -1 squared = 1)
dv <- function(v1, v2) {
  res <- sum((v1 - v2) ^ 2)
  return(res)
}

dv_norm <- function(v1, v2) {
  return(dv(v1,v2) / sqrt(sum(v1) * sum(v2)))
}
# helper functions from package `ape'

sortIndex <- function(X)
{
  if (length(X) == 1)
    return(1)
  if (!is.matrix(X))
    X <- as.matrix(X)
  apply(X, 2, function(v)
    order(rank(v)))
}
mst <- function(X)
{
  if (class(X) == "dist")
    X <- as.matrix(X)
  n <- dim(X)[1]
  N <- matrix(0, n, n)
  tree <- NULL
  large.value <- max(X) + 1
  diag(X) <- large.value
  index.i <- 1
  for (i in 1:(n - 1)) {
    tree <- c(tree, index.i)
    m <- apply(as.matrix(X[, tree]), 2, min)
    a <- sortIndex(X[, tree])[1,]
    b <- sortIndex(m)[1]
    index.j <- tree[b]
    index.i <- a[b]
    N[index.i, index.j] <- 1
    N[index.j, index.i] <- 1
    for (j in tree) {
      X[index.i, j] <- large.value
      X[j, index.i] <- large.value
    }
  }
  dimnames(N) <- dimnames(X)
  class(N) <- "mst"
  return(N)
}

# from package "cccd"
nng <- function (x = NULL, dx = NULL, k = 3, mutual = FALSE, method = NULL, 
    use.fnn = FALSE, algorithm = "cover_tree") 
{
    if (use.fnn) {
        if (is.null(x)) 
            stop("x must not be null. try use.fnn=FALSE")
        dx <- get.knn(x, k = k, algorithm = algorithm)
        edges <- matrix(unlist(sapply(1:nrow(x), function(i) {
            rbind(rep(i, k), dx$nn.index[i, ])
        })), nrow = 2)
        n <- nrow(x)
        if (mutual) {
            a <- apply(edges, 2, sort)
            b <- which(duplicated(a, MARGIN = 2))
            if (length(b) == 0) {
                out <- graph.empty(n, directed = FALSE)
            }
            else {
                out <- graph(edges[, b, drop = FALSE], n = n, 
                  directed = FALSE)
            }
        }
        else {
            out <- graph(edges, n = n, directed = TRUE)
        }
    }
    else {
        if (is.null(dx)) {
            if (is.null(x)) 
                stop("one of x or dx must be given")
            dx <- as.matrix(proxy::dist(x, method = method))
        }
        n <- nrow(dx)
        A <- matrix(0, nrow = n, ncol = n)
        for (i in 1:n) {
            d <- sort(dx[i, ])[-1]
            A[i, dx[i, ] <= d[k]] <- 1
        }
        diag(A) <- 0
        if (mutual) {
            for (i in 1:n) {
                A[i, ] <- A[i, ] & A[, i]
                A[, i] <- A[i, ]
            }
        }
        if (mutual) 
            out <- graph.adjacency(A, mode = "undirected")
        else out <- graph.adjacency(A, mode = "directed")
    }
    out$k <- k
    out$mutual <- mutual
    if (!is.null(x)) {
        out$layout <- x
    }
    out
}

# Add an argument to specify what kind of graph to use to connect the observations

gsg <- function(n0 = 0.05*length(graphs), n = 0.95*length(graphs), distance_n = "dv", grapher_n = "mst", ...) {
  # get functions corresponding to string arguments; could also use `do.call`
  distance <- match.fun(distance_n)
  grapher <- match.fun(grapher_n)
  #get binary adjacency matrices, vectorized, from graphs:
  v <- lapply(graphs[n0:n], function(g) {
    as.vector(get.adjacency(g))
  })
  d <-
    lapply(1:(length(v) - 1), function(i) {
      # for each i from 1 to n-1...
      lapply((i + 1):length(v), # compute the distance
             # of v_i from v_i+1...v_n
             function(j) {
               return(distance(v[[i]],v[[j]]))
             })
    })
  #this gets us the upper triangular matrix in vectorized form, diagonal removed
  #equal to 0 by definition (dist(v_i, v_i) must be 0)for metric to be
  #well defined:
  #length(unlist(d1)) == length(v)*(length(v)-1)/2
  d <- unlist(d)
  #empty matrix
  ut <- matrix(0, length(v), length(v))
  #fill the upper triangular
  ut[upper.tri(ut, diag = F)] <- d
  #full symmetrical distance matrix
  dm <- ut + t(ut)
  adj<- grapher(dm)
  if(grapher_n == 'nng'){
    adj <- get.adjacency(adj)
  }
  #make the graph
   g<-
    graph_from_adjacency_matrix(adj, mode = "undirected", diag = F)
  #get edgelist to apply gSeg functions (a matrix where each row is an edge,
  #on the columns the coordinates of that edge)

  el <- get.edgelist(g)
  gsg <- gseg1(length(v), el)
  plot(gsg$Z, type = "l", col = 'blue', main = "", ylab=(expression("Z[G](t)")))
}
```

Below we show in two different visualization how Chen's method behaves on our data set
with the distance measure we chose. Taking into account the high variance in the
size of the subject set, we chose to consider only the normalized 
Frobenius norm. We present results derived using a Minimum Spanning Tree and
a Nearest Neighbor graph to compare subgroups. Recall that Nearest Neighbor
Graph of order $K$ is the graph that connect each vertex to the closest $K$ other
vertices. Results are visible in \autoref{gbased}.

```{r gbasedcpd1, echo =F,message=F, fig.cap="MST, whole observation period considered", cache=T}
gsg(grapher_n = "mst")
```

```{r echo =F,message=F, fig.cap="NNG, whole observation period considered", cache=T}
gsg(grapher_n = "nng")
```

Both variants find a significant change-point at around $t=120$. This corresponds to about
the 20th of January in our case. Interestingly, in their application, @chen2015graph
attributed the change-point they found to vacation periods planned in the academic calendar.
Given that the asymptotic distribution of the scan statistic was shown by
@chen2015graph to be Gaussian, a value as high as the one reported implies a statistically
significant changepoint, with a $p$-value well below the $0.05$ threshold.

It has to be noted, however, that when using the Minimum Spanning Tree
as a reference measure the method's result appear to be very unstable. 
It is enough to vary slightly $n_0$ and $n_1$ (that is,
the period in which to search for the change-point) to lose the peak
observed around time 120. The variant using the Nearest Neighbor Graph appear
more consistent and results are stable to variations in the search limits.

A comparison between various parameter choices can be more efficiently carried
out in the **shiny** app than by showing dozens of plots in this work.A plot of the method
results obtained trimming the first and last 15\% of the observations is nonetheless
reported \autoref{cpdtrimmed} and \ref{cpdtrimmed2}. This decision was motivated by 
the gradual increase of activity at the start of experiment and the gradual decrease
at its end. Excluding the extremes, the activity in the network was more stable
and could be considered to be more representative of actual social connections
between subjects.

```{r gbasedcpdtr, echo =F, message=F, fig.cap="\\label{cpdtrimmed}MST, trimmed first and last 15% of obs"}
gsg(grapher_n = "mst", n0 = .15*length(graphs), n=.85*length(graphs))
```


```{r echo =F,message=F, cache = T, fig.cap="\\label{cpdtrimmed2}NNG, trimmed first and last 15% of obs"}
gsg(grapher_n = "nng", n0 = .15*length(graphs), n=.85*length(graphs))
```

Notice how in this second case the method identifies no statistically significant change-point when using the minimum spanning tree as
a reference measure. The maximum value of the scan statistic is around
1.44, below the statistically significant threshold, and it occurs at
$t=82$. The test, when performed using the nearest neighbor graph, gives
a consistent result.

To find evidence of a structural change, if it exists, we produced an
animated plot, of the type called by @beck2014state a *flip-book* animation. In
our example, the position of the vertices remains constant, and we chose to color
the edges and make them wider as they were repeated over time. We built this
representation with no specialized tools, simply combining the **animation** and
**igraph** packages.

We show, as an example, the graphs obtained for day
100 and 150 in \autoref{fig}. The full "movie", of course, can be re-obtained 
executing the code available at the source listed in the appendix.

```{r echo=F, message=F, fig.scap="Comparison of network snapshot before and after $\tau$", fig.cap="\\label{fig}Network at days 100 (left) and 150 (right) from the start of the experiment, with edges plotted as to highlight the relative strength of ties", fig.align='center'}
pal<- (brewer.pal(7, "Reds"))

par(mfrow=c(1,2))
# make complete animation:
    g <- cumgraphs[[100]];
    plot(g,
         layout=layout_in_circle(g,order=sort(V(g)$name)),
    #vertex.color=V(g)$year,
    edge.color=pal[E(g)$weight],
    vertex.color='light grey', 
    edge.width=E(g)$weight-1, # so the calls older than 2 days are not plotted
    vertex.label="", vertex.size=7)
    

    g <- cumgraphs[[150]];
    plot(g,
         layout=layout_in_circle(g,order=sort(V(g)$name)),
    #vertex.color=V(g)$year,
    edge.color=pal[E(g)$weight],
    vertex.color='light grey',
    edge.width=E(g)$weight-1, # so the calls older than 2 days are not plotted
    vertex.label=NA, vertex.size=7)
    
```

Interestingly enough, such a cursory look at the animation show no particular
change in the structure of interactions captured by the network, aside from a
gradual decrease in activity that was noticeable also from \autoref{missing},
so apparently the method was not able to identify reliably changepoints in the
data that could not be also inferred from the visual analysis of graphs
and simple time-series plots.

We must stress, however, that finding a dissimilarity measure for graphs
that captures intuition well and is at the same time sensitive to structural
changes is very difficult, and must rely on specific domain knowledge depending
on the field of application.

To conclude the report of our analysis, we were unable to identify a clear
changepoint the SocialEvolution experiment data. This result should not
be surprising, as we have shown and documented that even slight variations in the details of the test implementation can give 
results that are very different, thus highlighting a possible
shortcoming of the method. In that regard, further research
is needed, possibly by measuring the robustness of the results
to changes in the definition of distance and in the graph used
as a reference measure.
